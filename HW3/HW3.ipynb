{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste your implementations right here to check your result\n",
    "# (Of course you can add your classes not written here)\n",
    "\n",
    "\n",
    "def get_prob(sequence: np.ndarray, classes: np.ndarray) -> tuple[float, float]:\n",
    "    size = sequence.shape[0]\n",
    "    class_1 = np.count_nonzero(sequence == classes[0])\n",
    "    class_2 = np.count_nonzero(sequence == classes[1])\n",
    "\n",
    "    return class_1 / size, class_2 / size\n",
    "\n",
    "\n",
    "CriterionFunction = Callable[[np.ndarray], float]\n",
    "\n",
    "\n",
    "def gini(sequence: np.ndarray) -> float:\n",
    "    if len(np.unique(sequence)) < 2:\n",
    "        return 0\n",
    "    p1, p2 = get_prob(sequence, np.unique(sequence))\n",
    "    return 1 - np.square(p1) - np.square(p2)\n",
    "\n",
    "\n",
    "def entropy(sequence: np.ndarray) -> float:\n",
    "    if len(np.unique(sequence)) < 2:\n",
    "        return 0\n",
    "    p1, p2 = get_prob(sequence, np.unique(sequence))\n",
    "\n",
    "    return -(p1 * np.log2(p1) + p2 * np.log2(p2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 21)\n",
      "(300, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1583</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>148</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>942</td>\n",
       "      <td>1651</td>\n",
       "      <td>1704</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.8</td>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>1538</td>\n",
       "      <td>2459</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.7</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>1504</td>\n",
       "      <td>1799</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>164</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>873</td>\n",
       "      <td>1394</td>\n",
       "      <td>1944</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>695</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.6</td>\n",
       "      <td>196</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1649</td>\n",
       "      <td>1829</td>\n",
       "      <td>2855</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0           1583     1          2.1         1  11       0          14    0.7   \n",
       "1            745     1          0.6         1   5       0          35    0.8   \n",
       "2            832     0          0.7         1   2       1          39    0.7   \n",
       "3           1175     1          1.3         0   2       0          19    0.3   \n",
       "4            695     0          0.5         0  18       1          12    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        148        7  ...        942      1651  1704    17    13          2   \n",
       "1        102        8  ...         89      1538  2459    14     1         16   \n",
       "2        103        4  ...        125      1504  1799     5     2         11   \n",
       "3        164        7  ...        873      1394  1944     9     4          9   \n",
       "4        196        2  ...       1649      1829  2855    16    13          7   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        1             0     1            1  \n",
       "1        1             1     0            0  \n",
       "2        1             0     1            0  \n",
       "3        1             1     0            0  \n",
       "4        1             1     1            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df = pd.read_csv(\"val.csv\")\n",
    "\n",
    "\n",
    "x_train: np.ndarray = train_df.loc[:, train_df.columns != \"price_range\"].to_numpy()  # type: ignore\n",
    "y_train: np.ndarray = train_df.loc[:, \"price_range\"].to_numpy()\n",
    "x_val: np.ndarray = val_df.loc[:, train_df.columns != \"price_range\"].to_numpy()  # type: ignore\n",
    "y_val: np.ndarray = val_df.loc[:, \"price_range\"].to_numpy()\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    feature: Optional[int] = None\n",
    "    threshold: Optional[float] = None\n",
    "    l_node: Optional[\"Node\"] = None\n",
    "    r_node: Optional[\"Node\"] = None\n",
    "    mark: Optional[int] = None\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.mark is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples, n_train_feats = train_df.shape\n",
    "\n",
    "\n",
    "def convert_vote(data: np.ndarray) -> np.ndarray:\n",
    "    def replacer(x: int):\n",
    "        return -1 if x == 0 else x\n",
    "\n",
    "    return np.vectorize(replacer)(data)\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self, criterion=\"gini\", max_depth=None, negative=False, max_features=None\n",
    "    ):\n",
    "        self.criterion = {\"gini\": gini, \"entropy\": entropy}[criterion]\n",
    "        self.max_depth = max_depth\n",
    "        self.importance = np.zeros(n_train_feats - 1)\n",
    "        self.negative = negative\n",
    "        self.max_features = max_features\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "    def get_accuracy(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        return np.count_nonzero(y_pred == y_true) / len(y_pred)\n",
    "\n",
    "    def fit(self, x_data: np.ndarray, y_data: np.ndarray):\n",
    "        self.root = self.build_tree(x_data, y_data)\n",
    "\n",
    "    def predict(self, x_data: np.ndarray) -> np.ndarray:\n",
    "        pred = np.array([self.traverse_tree(x, self.root) for x in x_data])\n",
    "        if self.negative:\n",
    "            return convert_vote(pred)\n",
    "        return pred\n",
    "\n",
    "    def traverse_tree(self, x: np.ndarray, node: Node):\n",
    "        if node.is_leaf():\n",
    "            return node.mark\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.traverse_tree(x, node.l_node)  # type: ignore\n",
    "\n",
    "        return self.traverse_tree(x, node.r_node)  # type: ignore\n",
    "\n",
    "    def build_tree(\n",
    "        self, x_data: np.ndarray, y_data: np.ndarray, depth: int = 0\n",
    "    ) -> Node:\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or (\n",
    "            len(np.unique(y_data)) < 2\n",
    "        ):\n",
    "            leaf_mark = self.class_mark(y_data)\n",
    "            return Node(mark=leaf_mark)\n",
    "\n",
    "        feat_index, thres = self.best_split(x_data, y_data)\n",
    "        l_index, r_index = self.split(x_data[:, feat_index], thres)\n",
    "\n",
    "        current_uncertainty = self.criterion(y_data)\n",
    "        info = self.info_gain(\n",
    "            current_uncertainty,\n",
    "            y_data[l_index],\n",
    "            y_data[r_index],\n",
    "        )\n",
    "\n",
    "        if info == 0:\n",
    "            leaf_mark = self.class_mark(y_data)\n",
    "            return Node(mark=leaf_mark)\n",
    "        left = self.build_tree(x_data[l_index], y_data[l_index], depth + 1)\n",
    "        right = self.build_tree(x_data[r_index], y_data[r_index], depth + 1)\n",
    "\n",
    "        self.importance[feat_index] += 1\n",
    "        return Node(feature=feat_index, threshold=thres, l_node=left, r_node=right)\n",
    "\n",
    "    def split(self, feat_col: np.ndarray, thres: int):\n",
    "        l_index = np.argwhere(feat_col <= thres).flatten()\n",
    "        r_index = np.argwhere(feat_col > thres).flatten()\n",
    "\n",
    "        return l_index, r_index\n",
    "\n",
    "    def best_split(self, x_data: np.ndarray, y_data: np.ndarray):\n",
    "        n_feat = x_data.shape[1]\n",
    "\n",
    "        if self.max_features is not None:\n",
    "            feats = self.rng.choice(n_feat, size=self.max_features, replace=False)\n",
    "        else:\n",
    "            feats = range(n_feat)\n",
    "\n",
    "        best_error = 1\n",
    "        split_index, split_thres = 0, 0\n",
    "        for feat_index in feats:  # type: ignore\n",
    "            feat_col = x_data[:, feat_index]\n",
    "            thresholds = np.unique(feat_col)\n",
    "            for thres in thresholds:\n",
    "                error = self.calculate_error(x_data, y_data, feat_col, thres)\n",
    "\n",
    "                if error < best_error:\n",
    "                    best_error = error\n",
    "                    split_index = feat_index\n",
    "                    split_thres = thres\n",
    "\n",
    "        return split_index, split_thres\n",
    "\n",
    "    def info_gain(self, current: float, left: np.ndarray, right: np.ndarray):\n",
    "        p = len(left) / (len(left) + len(right))\n",
    "        return current - p * self.criterion(left) - (1 - p) * self.criterion(right)\n",
    "\n",
    "    def calculate_error(\n",
    "        self,\n",
    "        x_data: np.ndarray,\n",
    "        y_data: np.ndarray,\n",
    "        feat_col: np.ndarray,\n",
    "        thres: int,\n",
    "    ) -> float:\n",
    "        n_samples, _ = x_data.shape\n",
    "        l_index, r_index = self.split(feat_col, thres)\n",
    "        if len(l_index) == 0 or len(r_index) == 0:\n",
    "            return 1\n",
    "        l_criterion = self.criterion(y_data[l_index])\n",
    "        r_criterion = self.criterion(y_data[r_index])\n",
    "        criterion_index = l_criterion * (l_index.shape[0] / n_samples) + r_criterion * (\n",
    "            r_index.shape[0] / n_samples\n",
    "        )\n",
    "\n",
    "        return criterion_index\n",
    "\n",
    "    def class_mark(self, y_data: np.ndarray):\n",
    "        mark = np.argmax(np.bincount(y_data))\n",
    "        return int(mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_depth3 acc: 0.9166666666666666\n",
      "clf_depth10 acc: 0.9366666666666666\n"
     ]
    }
   ],
   "source": [
    "clf_depth3 = DecisionTree(criterion=\"gini\", max_depth=3)\n",
    "clf_depth3.fit(x_train, y_train)\n",
    "y_pred = clf_depth3.predict(x_val)\n",
    "acc = clf_depth3.get_accuracy(y_pred, y_val)\n",
    "print(f\"clf_depth3 acc: {acc}\")\n",
    "\n",
    "clf_depth10 = DecisionTree(criterion=\"gini\", max_depth=10)\n",
    "clf_depth10.fit(x_train, y_train)\n",
    "y_pred = clf_depth10.predict(x_val)\n",
    "acc = clf_depth10.get_accuracy(y_pred, y_val)\n",
    "print(f\"clf_depth10 acc: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_gini acc: 0.9166666666666666\n",
      "clf_entropy acc: 0.93\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion=\"gini\", max_depth=3)\n",
    "clf_gini.fit(x_train, y_train)\n",
    "y_pred = clf_gini.predict(x_val)\n",
    "acc = clf_gini.get_accuracy(y_pred, y_val)\n",
    "print(f\"clf_gini acc: {acc}\")\n",
    "\n",
    "clf_entropy = DecisionTree(criterion=\"entropy\", max_depth=3)\n",
    "clf_entropy.fit(x_train, y_train)\n",
    "y_pred = clf_entropy.predict(x_val)\n",
    "acc = clf_entropy.get_accuracy(y_pred, y_val)\n",
    "print(f\"clf_entropy acc: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(x_train.columns.to_list())\n",
    "plt.barh(\n",
    "    range(clf_depth10.importance.shape[0], 0, -1),\n",
    "    clf_depth10.importance,\n",
    "    tick_label=train_df.loc[:, train_df.columns != \"price_range\"].columns,\n",
    "    height=0.5,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.clfs: list[DecisionTree] = []\n",
    "        self.alpha: list[float] = []\n",
    "\n",
    "    def fit(self, x_data: np.ndarray, y_data: np.ndarray):\n",
    "        self.clfs = []\n",
    "        y_series = convert_vote(y_data)\n",
    "        n_samples = x_data.shape[0]\n",
    "        w = np.full(n_samples, 1 / n_samples)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            weighted_index = np.random.choice(n_samples, n_samples, p=w)\n",
    "            weighted_x_train = x_data[weighted_index]\n",
    "            weighted_y_train = y_data[weighted_index]\n",
    "\n",
    "            clf = DecisionTree(criterion=\"gini\", max_depth=1, negative=True)\n",
    "            clf.fit(weighted_x_train, weighted_y_train)\n",
    "            y_pred = clf.predict(x_data)\n",
    "\n",
    "            error = self.loss_function(y_pred, y_series, w)\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            if error > 0.5:\n",
    "                self.alpha.append(-alpha)\n",
    "            else:\n",
    "                self.alpha.append(alpha)\n",
    "\n",
    "            self.clfs.append(clf)\n",
    "            w = w * np.exp(-alpha * y_pred * y_series)\n",
    "            w = w / np.sum(w)\n",
    "\n",
    "    def predict(self, x_data: np.ndarray):\n",
    "        y_pred = np.zeros(x_data.shape[0])\n",
    "        for clf, alpha in zip(self.clfs, self.alpha):\n",
    "            y = clf.predict(x_data)\n",
    "            y = y * alpha\n",
    "            y_pred = y_pred + y\n",
    "\n",
    "        return y_pred > 0\n",
    "\n",
    "    def loss_function(self, y_pred: np.ndarray, y_data: np.ndarray, w: np.ndarray):\n",
    "        error = w * (y_pred != y_data)\n",
    "        return np.sum(error)\n",
    "\n",
    "    def get_accuracy(self, y_pred: np.ndarray, y_true: np.ndarray):\n",
    "        return np.count_nonzero(y_pred == y_true) / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_10 acc: 0.9366666666666666\n"
     ]
    }
   ],
   "source": [
    "clf_10 = AdaBoost(10)\n",
    "clf_10.fit(x_train, y_train)\n",
    "y_pred = clf_10.predict(x_val)\n",
    "acc = clf_10.get_accuracy(y_pred, y_val)\n",
    "print(f\"clf_10 acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_100 acc: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "clf_100 = AdaBoost(100)\n",
    "clf_100.fit(x_train, y_train)\n",
    "y_pred = clf_100.predict(x_val)\n",
    "acc = clf_100.get_accuracy(y_pred, y_val)\n",
    "print(f\"clf_100 acc: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators,\n",
    "        max_features,\n",
    "        bootstrap=True,\n",
    "        criterion=\"gini\",\n",
    "        max_depth=None,\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = int(max_features)\n",
    "        self.bootstrap = bootstrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.clfs: list[DecisionTree] = []\n",
    "\n",
    "    def fit(self, x_data: np.ndarray, y_data: np.ndarray):\n",
    "        self.clfs = []\n",
    "        generator = np.random.default_rng()\n",
    "        n_samples = x_data.shape[0]\n",
    "        if self.bootstrap:\n",
    "            data_index = generator.choice(n_samples, n_samples)\n",
    "            x_data = x_data[data_index]\n",
    "            y_data = y_data[data_index]\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            clf = DecisionTree(\n",
    "                criterion=self.criterion,\n",
    "                max_depth=self.max_depth,\n",
    "                max_features=self.max_features,\n",
    "                negative=True,\n",
    "            )\n",
    "            clf.fit(x_data=x_data, y_data=y_data)\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, x_data: np.ndarray):\n",
    "        vote = np.zeros(x_data.shape[0])\n",
    "        i = 0\n",
    "        for clf in self.clfs:\n",
    "            i += 1\n",
    "            y_pred = clf.predict(x_data=x_data)\n",
    "            vote = vote + y_pred\n",
    "\n",
    "        return vote > 0\n",
    "\n",
    "    def get_accuracy(self, y_pred: np.ndarray, y_true: np.ndarray):\n",
    "        return np.count_nonzero(y_pred == y_true) / len(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_10tree acc: 0.9166666666666666\n",
      "clf_100tree acc: 0.9233333333333333\n"
     ]
    }
   ],
   "source": [
    "clf_10tree.fit(x_data=x_train, y_data=y_train)\n",
    "y_pred = clf_10tree.predict(x_data=x_val)\n",
    "acc = clf_10tree.get_accuracy(y_pred=y_pred, y_true=y_val)\n",
    "print(f\"clf_10tree acc: {acc}\")\n",
    "\n",
    "clf_100tree.fit(x_data=x_train, y_data=y_train)\n",
    "y_pred = clf_100tree.predict(x_data=x_val)\n",
    "acc = clf_100tree.get_accuracy(y_pred=y_pred, y_true=y_val)\n",
    "print(f\"clf_100tree acc: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_features = RandomForest(\n",
    "    n_estimators=10, max_features=np.sqrt(x_train.shape[1])\n",
    ")\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_random_features acc: 0.9233333333333333\n",
      "clf_all_features acc: 0.94\n"
     ]
    }
   ],
   "source": [
    "clf_random_features.fit(x_data=x_train, y_data=y_train)\n",
    "y_pred = clf_random_features.predict(x_data=x_val)\n",
    "acc = clf_random_features.get_accuracy(y_pred=y_pred, y_true=y_val)\n",
    "print(f\"clf_random_features acc: {acc}\")\n",
    "\n",
    "clf_all_features.fit(x_data=x_train, y_data=y_train)\n",
    "y_pred = clf_all_features.predict(x_data=x_val)\n",
    "acc = clf_all_features.get_accuracy(y_pred=y_pred, y_true=y_val)\n",
    "print(f\"clf_all_features acc: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Train and tune your model on a real-world dataset\n",
    "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_your_model(data):\n",
    "    ## Define your model and training\n",
    "    x_train: np.ndarray = data.loc[:, train_df.columns != \"price_range\"].to_numpy()  # type: ignore\n",
    "    y_train: np.ndarray = data.loc[:, \"price_range\"].to_numpy()\n",
    "\n",
    "    ada_100 = AdaBoost(50)\n",
    "    ada_100.fit(x_train, y_train)\n",
    "\n",
    "    return ada_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merge = pd.concat([train_df, val_df], ignore_index=True)\n",
    "my_model = train_your_model(train_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model acc: 0.9766666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = my_model.predict(x_val)\n",
    "acc = my_model.get_accuracy(y_pred, y_val)\n",
    "\n",
    "print(f\"my_model acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [312], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39massert\u001b[39;00m y_pred\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39m500\u001b[39m,)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert y_pred.shape == (500,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test = pd.read_csv(\"y_test.csv\")[\"price_range\"].values\n",
    "\n",
    "print(\"Test-set accuarcy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** We will check your result for Question 3 manually *** (5 points)\n",
      "*** We will check your result for Question 6 manually *** (20 points)\n",
      "Approximate score range: 45.0 ~ 70.0\n",
      "*** This score is only for reference ***\n"
     ]
    }
   ],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(\n",
    "    score, thres, CLS, kwargs, name, x_train, y_train, x_test, y_test, patient=10\n",
    "):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = (\n",
    "        \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    )\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\n",
    "            \"Length\",\n",
    "            \"Diameter\",\n",
    "            \"Height\",\n",
    "            \"Whole weight\",\n",
    "            \"Shucked weight\",\n",
    "            \"Viscera weight\",\n",
    "            \"Shell weight\",\n",
    "            \"Age\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df[\"Target\"] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df[\"Target\"].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df[\"Target\"].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(\n",
    "    5,\n",
    "    0.9337,\n",
    "    DecisionTree(criterion=\"gini\", max_depth=3),\n",
    "    \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    ")\n",
    "\n",
    "score += discrete_checker(\n",
    "    2.5,\n",
    "    0.9036,\n",
    "    DecisionTree(criterion=\"gini\", max_depth=10),\n",
    "    \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    ")\n",
    "\n",
    "score += discrete_checker(\n",
    "    2.5,\n",
    "    0.9096,\n",
    "    DecisionTree(criterion=\"entropy\", max_depth=3),\n",
    "    \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5,\n",
    "    0.91,\n",
    "    AdaBoost,\n",
    "    {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5,\n",
    "    0.87,\n",
    "    AdaBoost,\n",
    "    {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5,\n",
    "    0.91,\n",
    "    RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5,\n",
    "    0.91,\n",
    "    RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5,\n",
    "    0.92,\n",
    "    RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ebc8587b40ba206a63acdaa68892d3f2405780135fc7b6b13b25f1a0c680118"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
